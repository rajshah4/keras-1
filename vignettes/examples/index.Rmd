---
title: "Keras Examples"
---

| Example  | Description   |
|----------------------|------------------------------------|
| [addition_rnn.R](addition_rnn.html) | Implementation of sequence to sequence learning for performing addition of two numbers (as strings). |
| [babi_memnn.R](babi_memnn.html) | Trains a memory network on the bAbI dataset for reading comprehension. |
| [babi_rnn.R](babi_rnn.html) | Trains a two-branch recurrent network on the bAbI dataset for reading comprehension. |
| [cifar10_cnn.R](cifar10_cnn.html) | Trains a simple deep CNN on the CIFAR10 small images dataset. |
| [imdb_cnn.R](imdb_cnn.html) | Demonstrates the use of Convolution1D for text classification. |
| [imdb_cnn_lstm.R](imdb_cnn_lstm.html) | Trains a convolutional stack followed by a recurrent stack network on the IMDB sentiment classification task. |
| [mnist_transfer_cnn.R](mnist_transfer_cnn.html) | Transfer learning toy example. |

<!--
| [conv_filter_visualization.R](conv_filter_visualization.html) | Visualization of the filters of VGG16, via gradient ascent in input space. |
| [conv_lstm.R](conv_lstm.html) | Demonstrates the use of a convolutional LSTM network. |
| [deep_dream.R](deep_dream.html) | Deep Dreams in Keras. |
| [image_ocr.R](image_ocr.html) | Trains a convolutional stack followed by a recurrent stack and a CTC logloss function to perform optical character recognition (OCR). |
| [imdb_bidirectional_lstm.R](imdb_bidirectional_lstm.html) | Trains a Bidirectional LSTM on the IMDB sentiment classification task. |
| [imdb_fasttext.R](imdb_fasttext.html) | Trains a FastText model on the IMDB sentiment classification task. |
| [imdb_lstm.R](imdb_lstm.html) | Trains a LSTM on the IMDB sentiment classification task. |
| [lstm_benchmark.R](lstm_benchmark.html) | Compares different LSTM implementations on the IMDB sentiment classification task. |
| [lstm_text_generation.R](lstm_text_generation.html) | Generates text from Nietzsche's writings. |
| [mnist_acgan.R](mnist_acgan.html) | Implementation of AC-GAN ( Auxiliary Classifier GAN ) on the MNIST dataset |
| [mnist_cnn.R](mnist_cnn.html) | Trains a simple convnet on the MNIST dataset. |
| [mnist_hierarchical_rnn.R](mnist_hierarchical_rnn.html) | Trains a Hierarchical RNN (HRNN) to classify MNIST digits. |
| [mnist_irnn.R](mnist_irnn.html) | Reproduction of the IRNN experiment with pixel-by-pixel sequential MNIST in "A Simple Way to Initialize Recurrent Networks of Rectified Linear Units" by Le et al. |
| [mnist_mlp.R](mnist_mlp.html) | Trains a simple deep multi-layer perceptron on the MNIST dataset. |
| [mnist_net2net.R](mnist_net2net.html) | Reproduction of the Net2Net experiment with MNIST in "Net2Net: Accelerating Learning via Knowledge Transfer". |
| [mnist_siamese_graph.R](mnist_siamese_graph.html) | Trains a Siamese multi-layer perceptron on pairs of digits from the MNIST dataset. |
| [mnist_swwae.R](mnist_swwae.html) | Trains a Stacked What-Where AutoEncoder built on residual blocks on the MNIST dataset. |
| [neural_doodle.R](neural_doodle.html) | Neural doodle. |
| [neural_style_transfer.R](neural_style_transfer.html) | Neural style transfer. |
| [pretrained_word_embeddings.R](pretrained_word_embeddings.html) | Loads pre-trained word embeddings (GloVe embeddings) into a frozen Keras Embedding layer, and uses it to train a text classification model on the 20 Newsgroup dataset. |
| [reuters_mlp.R](reuters_mlp.html) | Trains and evaluatea a simple MLP on the Reuters newswire topic classification task. |
| [stateful_lstm.R](stateful_lstm.html) | Demonstrates how to use stateful RNNs to model long sequences efficiently. |
| [variational_autoencoder.R](variational_autoencoder.html) | Demonstrates how to build a variational autoencoder. |
| [variational_autoencoder_deconv.R](variational_autoencoder_deconv.html) | Demonstrates how to build a variational autoencoder with Keras using deconvolution layers. |
-->














